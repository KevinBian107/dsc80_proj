{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framing Prediction Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "from itertools import chain\n",
    "\n",
    "from utils.eda import *\n",
    "from utils.dsc80_utils import *\n",
    "from utils.graph import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_csv('food_data/RAW_interactions.csv')\n",
    "recipes = pd.read_csv('food_data/RAW_recipes.csv')\n",
    "step0 = recipes.merge(interactions, how='left', left_on='id', right_on='recipe_id', indicator=True)\n",
    "df = (step0\n",
    "      .pipe(initial)\n",
    "      .pipe(transform_df)\n",
    "      #.pipe(outlier)\n",
    "      .pipe(group_recipe)\n",
    "      #.pipe(group_user)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Identification\n",
    "**Analysis**:\n",
    "Identify a prediction problem. Feel free to use one of the example prediction problems stated in the “Example Questions and Prediction Problems” section of your dataset’s description page or pose a hypothesis test of your own. The prediction problem you come up with doesn’t have to be related to the question you were answering in Steps 1-4, but ideally, your entire project has some sort of coherent theme.\n",
    "\n",
    "**Report**:\n",
    "Clearly state your prediction problem and type (classification or regression). If you are building a classifier, make sure to state whether you are performing binary classification or multiclass classification. Report the response variable (i.e. the variable you are predicting) and why you chose it, the metric you are using to evaluate your model and why you chose it over other suitable metrics (e.g. accuracy vs. F1-score).\n",
    "\n",
    "Note: Make sure to justify what information you would know at the “time of prediction” and to only train your model using those features. For instance, if we wanted to predict your final exam grade, we couldn’t use your Project 4 grade, because Project 4 is only due after the final exam! Feel free to ask questions if you’re not sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Potential Ideas:\n",
    "1. Sentiment Analysis with `review` column\n",
    "\n",
    "2. Using   `recipe` column and feature engineering (length of `recipe`, TF-IDF, ...) to predict `ratings`\n",
    "\n",
    "3. Using text data as a input to predict the rating of the user and identify preference of users (pre-step to reconmender system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['minutes', 'n_steps', 'n_ingredients', 'avg_rating', 'rating',\n",
       "       'calories', 'total_fat', 'sugar', 'sodium', 'protein', 'sat_fat',\n",
       "       'carbs', 'steps', 'name', 'description', 'ingredients', 'user_id',\n",
       "       'contributor_id', 'review_date', 'review', 'recipe_date', 'tags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Framing Question\n",
    "We know that Recipe's mean TFIDF distribution is different for higher rating recipe than lower rating recipe:\n",
    "- We need `X` and a `y` -> find relationships! -> Supervised ML model\n",
    "- We currently have the DataFrame grouped by recipe\n",
    "- We want to predict `rating` as a classfication problem\n",
    "    - `rating` in recipe df: a quality of recipe\n",
    "    - `rating` in user_id df: user preference ✅\n",
    "- Features for user_id df:\n",
    "    - `TF-IDF mean/max/sum/partial_mean` of `description` for **recipe per user_id** (may have more than one recipe) that have **high ratings**\n",
    "        - This evaluates whether a word shows more often in this **user's high rated recipe decription** compare to all **recipe decription**, thus, meaning that it is more important to this user.\n",
    "    - `n_ingredients`\n",
    "    - `n_steps`\n",
    "    - `minutes`\n",
    "    - `calories`\n",
    "    - `sodium`\n",
    "    - `previous_rating` (need to explore)\n",
    "    - `word2vec` (need to explore, somr info [here](https://towardsdatascience.com/word2vec-explained-49c52b4ccb71)) \n",
    "        - Each `user_id` have a pool of words in a **vector space** (from description, can have more)\n",
    "        - We want to see how similar (cosine distance) between recipe tags `word2vec` and the pool\n",
    "        - [good theory background](https://medium.com/@zafaralibagh6/a-simple-word2vec-tutorial-61e64e38a6a1)\n",
    "\n",
    "- consider using `tags`, `review`, `steps`?\n",
    "\n",
    "- Further: using preference to recomand recipe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word2Vec NLP\n",
    "[This](https://arxiv.org/pdf/1301.3781.pdf) is the original paper published by Google Research\n",
    "\n",
    "<center><img src=\"imgs/wv0.webp\" width=50%></center>\n",
    "\n",
    "\n",
    "We need to create a vocabulary of all the words in our text and then to encode our word as a vector of the same dimensions of our vocabulary, so this is exactly using `OneHotEncoding`, this input is given to a neural network with a single hidden layer.\n",
    "\n",
    "<center><img src=\"imgs/wv1.webp\" width=30%></center>\n",
    "\n",
    "The output of such network is a **single vector (also with the same length components) containing** that  represents the **probability** that a randomly selected nearby word is that vocabulary word.\n",
    "\n",
    "In word2vec, a distributed representation of a word is used. Take a vector with several hundred dimensions (say 1000). Each word is represented by a distribution of weights across those elements. So instead of a one-to-one mapping between an element in the vector and a word, **the representation of a word is spread across all the elements in the vector**, and **each element in the vector contributes to the definition of many words**. Such a vector comes to represent in some abstract way the ‘meaning’ of a word.\n",
    "- It is not one word that represenst one word but rather \"all\" words that represents one word\n",
    "- This is the distributional semantics idea, using an distribution of an sentence to encode/embed the meaning of an word\n",
    "\n",
    "\n",
    "<center><img src=\"imgs/wv2.webp\" width=50%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW (Continuous Bag of Words) & Continuous Skip-Grams\n",
    "So that is the basic Idea, how does it achieve such magical embedding?\n",
    "\n",
    "In the `CBOW model`, the **distributed representations of context** (or surrounding words) are combined to **predict the word in the middle**. While in the `Skip-gram model`, the **distributed representation of the input word** is used to predict the **context**.\n",
    "\n",
    "<center><img src=\"imgs/wv3.png\" width=70%></center>\n",
    "\n",
    "### In `CBOW`\n",
    "- Note that `CBOW` does not consider orders of the word, it takes in both \"past\" words and \"future\" words, just like in regular `bag of words` model. However, it uses continuous distributed representation of the context.\n",
    "\n",
    "- In `CBOW`, since our input vectors are `OneHotEncoding`, multiplying an input vector by the weight matrix `W1` is just selecting a row/word from `W1`. From the hidden layer to the output layer, the second weight matrix `W2` can be used to compute **a score** for each word in the vocabulary, and **softmax** can be used to obtain the **posterior distribution of words**.\n",
    "\n",
    "### In `Skip-Gram`\n",
    "- The `skip-gram model` is the opposite of the `CBOW` model. It is constructed with the **focus word as the single input vector**, and the target context words are now at the output layer. The activation function for the hidden layer simply amounts to copying the corresponding row from the weights matrix `W1` (linear) as we saw before. At the output layer, we now output **C multinomial distributions instead of just one**.\n",
    "\n",
    "- Given the sentence: *“I will have orange juice and eggs for breakfast.”*\n",
    "    - and a window size of 2, if the target word is juice, its neighboring words will be ( have, orange, and, eggs). Our input and target word pair would be (juice, have), (juice, orange), (juice, and), (juice, eggs).\n",
    "\n",
    "    - Also note that within the sample window, proximity of the words to the source word plays no role. So have, orange, and, and eggs will be treated the same while training.\n",
    "\n",
    "    - The dimensions of the input vector will be **1xV** — where V is the number of words in the vocabulary — i.e `OneHotEncoding` representation of the word. The single hidden layer will have dimension **VxE**, where E is the size of the word embedding and is a hyper-parameter. The output from the hidden layer would be of the dimension **1xE**, which we will feed into an `softmax` layer. The dimensions of the output layer will be 1xV, where each value in the vector will be *the probability score of the target word at that position*.\n",
    "\n",
    "<center><img src=\"imgs/window.webp\" width=50%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "# nltk.download('brown')\n",
    "# nltk.download('movie_reviews')\n",
    "# nltk.download('treebank')\n",
    "# from nltk.corpus import brown, movie_reviews, treebank\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = Word2Vec(brown.sents())\n",
    "# mr = Word2Vec(movie_reviews.sents())\n",
    "# t = Word2Vec(treebank.sents())\n",
    "# brown.sents()\n",
    "# b.wv.most_similar('food', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'of',\n",
       " 'my',\n",
       " \"mom's\",\n",
       " 'favorite',\n",
       " 'bisquick',\n",
       " 'recipes.',\n",
       " 'this',\n",
       " 'brings',\n",
       " 'back',\n",
       " 'memories!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus = ' '.join(df['description'].astype(str))\n",
    "# tokens = word_tokenize(corpus)\n",
    "# tokens\n",
    "tokens = df['description'].astype(str).str.split(' ').to_list()\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = Word2Vec(tokens, window=7, sg=1, min_count=3) # input is a list of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cheddar', 0.8366087079048157),\n",
       " ('cheese,', 0.8310773372650146),\n",
       " ('cheese.', 0.8167422413825989),\n",
       " ('cheese!', 0.7785608768463135),\n",
       " ('fontina', 0.7774136066436768),\n",
       " ('mozzarella', 0.7537940144538879),\n",
       " ('cheeses', 0.7433044910430908),\n",
       " ('havarti', 0.7396714091300964),\n",
       " ('cheddar.', 0.7392040491104126),\n",
       " ('monterey', 0.7332344055175781),\n",
       " ('gruyere', 0.7295688986778259),\n",
       " ('provolone', 0.7268673777580261),\n",
       " ('sharp', 0.726447343826294),\n",
       " ('cheese...', 0.7256677746772766),\n",
       " ('cheddar,', 0.7215008735656738),\n",
       " ('crumbled', 0.7210201621055603),\n",
       " ('asiago', 0.7161028981208801),\n",
       " ('jack.', 0.7128156423568726),\n",
       " ('romano', 0.7125892043113708),\n",
       " ('parmesan', 0.7068539261817932)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec.wv.most_similar('cheese', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinb/Desktop/dsc80/dsc80_proj/utils/eda.py:115: FutureWarning:\n",
      "\n",
      "Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user = (step0\n",
    "      .pipe(initial)\n",
    "      .pipe(transform_df)\n",
    "      #.pipe(outlier)\n",
    "      #.pipe(group_recipe)\n",
    "      .pipe(group_user)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67268,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_corpus = user['description'].astype(str).str.strip('[]').str.strip(\"'\").str.strip('\"').str.split(' ')\n",
    "user_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = Word2Vec(user_corpus, window=7, sg=1, min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cheddar', 0.7837750911712646),\n",
       " ('havarti', 0.742730438709259),\n",
       " ('cabot', 0.7361286878585815),\n",
       " ('parmesan', 0.732124924659729),\n",
       " ('cheese.', 0.7221015691757202),\n",
       " ('fontina', 0.7215815186500549),\n",
       " ('pepper-jack', 0.7188788652420044),\n",
       " ('cheddar.', 0.7154257893562317),\n",
       " ('cheese...', 0.7148705720901489),\n",
       " ('goat', 0.713663637638092),\n",
       " ('wontons.', 0.7099077105522156),\n",
       " ('maytag', 0.7073398232460022),\n",
       " ('feta', 0.704619288444519),\n",
       " ('romano', 0.7013906240463257),\n",
       " ('cream', 0.7006925344467163),\n",
       " (\"cheese.',\", 0.6994298696517944),\n",
       " ('tapenade,', 0.699428915977478),\n",
       " ('mac', 0.697147786617279),\n",
       " ('colby,', 0.6966875195503235),\n",
       " ('goats', 0.6960228681564331),\n",
       " ('mozzarella,', 0.694808840751648),\n",
       " ('cheez', 0.6945345997810364),\n",
       " ('frig:', 0.6922464370727539),\n",
       " ('crumbles.', 0.6880894303321838),\n",
       " ('http://www.recipezaar.com/fannie-farmers-classic-baked-macaroni-and-cheese-135350',\n",
       "  0.6870267987251282),\n",
       " ('provalone', 0.6867004632949829),\n",
       " ('chevre', 0.6864020824432373),\n",
       " ('whiz', 0.6849308609962463),\n",
       " ('transformation,', 0.6843330264091492),\n",
       " ('gruyere', 0.6842995882034302)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec.wv.most_similar('cheese', topn=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `distance` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7996206283569336"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec.wv.distance('food','delicious')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tags'].apply(lambda x: [word_vec.wv.distance(word) for word in x if word in word_vec.wv.vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
