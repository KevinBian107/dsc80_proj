{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word2Vec NLP\n",
    "[This](https://arxiv.org/pdf/1301.3781.pdf) is the original paper published by Google Research\n",
    "\n",
    "<center><img src=\"imgs/wv0.webp\" width=50%></center>\n",
    "\n",
    "\n",
    "We need to create a vocabulary of all the words in our text and then to encode our word as a vector of the same dimensions of our vocabulary, so this is exactly using `OneHotEncoding`, this input is given to a neural network with a single hidden layer.\n",
    "\n",
    "<center><img src=\"imgs/wv1.webp\" width=30%></center>\n",
    "\n",
    "The output of such network is a **single vector (also with the same length components) containing** that  represents the **probability** that a randomly selected nearby word is that vocabulary word.\n",
    "\n",
    "In word2vec, a distributed representation of a word is used. Take a vector with several hundred dimensions (say 1000). Each word is represented by a distribution of weights across those elements. So instead of a one-to-one mapping between an element in the vector and a word, **the representation of a word is spread across all the elements in the vector**, and **each element in the vector contributes to the definition of many words**. Such a vector comes to represent in some abstract way the ‘meaning’ of a word.\n",
    "- It is not one word that represenst one word but rather \"all\" words that represents one word\n",
    "- This is the distributional semantics idea, using an distribution of an sentence to encode/embed the meaning of an word\n",
    "\n",
    "\n",
    "<center><img src=\"imgs/wv2.webp\" width=50%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW (Continuous Bag of Words) & Continuous Skip-Grams\n",
    "So that is the basic Idea, how does it achieve such magical embedding?\n",
    "\n",
    "In the `CBOW model`, the **distributed representations of context** (or surrounding words) are combined to **predict the word in the middle**. While in the `Skip-gram model`, the **distributed representation of the input word** is used to predict the **context**.\n",
    "\n",
    "<center><img src=\"imgs/wv3.png\" width=70%></center>\n",
    "\n",
    "### In `CBOW`\n",
    "- Note that `CBOW` does not consider orders of the word, it takes in both \"past\" words and \"future\" words, just like in regular `bag of words` model. However, it uses continuous distributed representation of the context.\n",
    "\n",
    "- In `CBOW`, since our input vectors are `OneHotEncoding`, multiplying an input vector by the weight matrix `W1` is just selecting a row/word from `W1`. From the hidden layer to the output layer, the second weight matrix `W2` can be used to compute **a score** for each word in the vocabulary, and **softmax** can be used to obtain the **posterior distribution of words**.\n",
    "\n",
    "### In `Skip-Gram`\n",
    "- The `skip-gram model` is the opposite of the `CBOW` model. It is constructed with the **focus word as the single input vector**, and the target context words are now at the output layer. The activation function for the hidden layer simply amounts to copying the corresponding row from the weights matrix `W1` (linear) as we saw before. At the output layer, we now output **C multinomial distributions instead of just one**.\n",
    "\n",
    "- Given the sentence: *“I will have orange juice and eggs for breakfast.”*\n",
    "    - and a window size of 2, if the target word is juice, its neighboring words will be ( have, orange, and, eggs). Our input and target word pair would be (juice, have), (juice, orange), (juice, and), (juice, eggs).\n",
    "\n",
    "    - Also note that within the sample window, proximity of the words to the source word plays no role. So have, orange, and, and eggs will be treated the same while training.\n",
    "\n",
    "    - The dimensions of the input vector will be **1xV** — where V is the number of words in the vocabulary — i.e `OneHotEncoding` representation of the word. The single hidden layer will have dimension **VxE**, where E is the size of the word embedding and is a hyper-parameter. The output from the hidden layer would be of the dimension **1xE**, which we will feed into an `softmax` layer. The dimensions of the output layer will be 1xV, where each value in the vector will be *the probability score of the target word at that position*.\n",
    "\n",
    "<center><img src=\"imgs/window.webp\" width=50%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "from itertools import chain\n",
    "\n",
    "from utils.eda import *\n",
    "from utils.dsc80_utils import *\n",
    "from utils.graph import *\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "# nltk.download('brown')\n",
    "# nltk.download('movie_reviews')\n",
    "# nltk.download('treebank')\n",
    "# from nltk.corpus import brown, movie_reviews, treebank\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_csv('food_data/RAW_interactions.csv')\n",
    "recipes = pd.read_csv('food_data/RAW_recipes.csv')\n",
    "step0 = recipes.merge(interactions, how='left', left_on='id', right_on='recipe_id', indicator=True)\n",
    "df_recipe = (step0\n",
    "      .pipe(initial)\n",
    "      .pipe(transform_df)\n",
    "      #.pipe(outlier)\n",
    "      .pipe(group_recipe)\n",
    "      #.pipe(group_user)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = Word2Vec(brown.sents())\n",
    "# mr = Word2Vec(movie_reviews.sents())\n",
    "# t = Word2Vec(treebank.sents())\n",
    "# brown.sents()\n",
    "# b.wv.most_similar('food', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'of',\n",
       " 'my',\n",
       " \"mom's\",\n",
       " 'favorite',\n",
       " 'bisquick',\n",
       " 'recipes.',\n",
       " 'this',\n",
       " 'brings',\n",
       " 'back',\n",
       " 'memories!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus = ' '.join(df['description'].astype(str))\n",
    "# tokens = word_tokenize(corpus)\n",
    "# tokens\n",
    "tokens = df_recipe['description'].astype(str).str.split(' ').to_list()\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = Word2Vec(tokens, window=7, sg=1, min_count=3) # input is a list of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cheddar', 0.7908340692520142),\n",
       " ('goat', 0.7435830235481262),\n",
       " ('fontina', 0.737315833568573),\n",
       " ('goats', 0.731482982635498),\n",
       " ('parmesan', 0.7294240593910217),\n",
       " ('gruyere', 0.7245720624923706),\n",
       " ('montrachet', 0.722097635269165),\n",
       " ('romano', 0.7213541865348816),\n",
       " ('tillamook', 0.7174773812294006),\n",
       " ('havarti', 0.7120335102081299)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec.wv.most_similar('cheese', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinb/Desktop/dsc80/dsc80_proj/utils/eda.py:115: FutureWarning:\n",
      "\n",
      "Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user = (step0\n",
    "      .pipe(initial)\n",
    "      .pipe(transform_df)\n",
    "      #.pipe(outlier)\n",
    "      #.pipe(group_recipe)\n",
    "      .pipe(group_user)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67268,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_corpus = df_user['description'].astype(str).str.strip('[]').str.strip(\"'\").str.strip('\"').str.split(' ')\n",
    "user_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = Word2Vec(user_corpus, window=7, sg=1, min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cheddar', 0.7908340692520142),\n",
       " ('goat', 0.7435830235481262),\n",
       " ('fontina', 0.737315833568573),\n",
       " ('goats', 0.731482982635498),\n",
       " ('parmesan', 0.7294240593910217),\n",
       " ('gruyere', 0.7245720624923706),\n",
       " ('montrachet', 0.722097635269165),\n",
       " ('romano', 0.7213541865348816),\n",
       " ('tillamook', 0.7174773812294006),\n",
       " ('havarti', 0.7120335102081299)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec.wv.most_similar('cheese', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `distance` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7651446610689163"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec.wv.distance('food','delicious')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
